{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUXYC79N-qpv"
      },
      "source": [
        "# Integrantes del equipo Concentrados\n",
        "- Daniel Queijeiro Albo - A01710441\n",
        "- Diego Alfaro Pinto - A01709971\n",
        "- Diego Isaac Fuentes Juvera - A01705506\n",
        "- Jesus Ramirez Delgado - A01274723\n",
        "- Mauricio Anguiano Juarez - A01703337\n",
        "- Luis Adrián Uribe Cruz - A01783129"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8vSsua-qpz"
      },
      "source": [
        "## TC 3007B\n",
        "### GPT 2\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 2,3: Code GPT2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - To understand the Transformer architecture.\n",
        "    - To code GPT 2.\n",
        "    - To gain understanding of the LLMs' autoregresive nature..\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions\n",
        "\n",
        "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
        "\n",
        "    Follow the provided code. The code already implements a transformer from scratch as explained in [this video](https://youtu.be/51jq4wnHYaY)\n",
        "\n",
        "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  \n",
        "  \n",
        "- Evaluation Criteria\n",
        "\n",
        "    - Code Readability and Comments (40%).\n",
        "    - Traning a LM,  complete 'Train function' (30%).\n",
        "    - Generating at least 10 sentences, comple 'Sample function' (30%).\n",
        "\n",
        "- Submission\n",
        "\n",
        "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8uV9GQs-qp0"
      },
      "source": [
        "# Contribuciones individuales\n",
        "Daniel Queijeiro - Buscar datasets e importarlos para el entrenamiento\n",
        "Diego Fuentes - Proponer el tokenizer Tiktoken para experimentar y mejorar el modelo\n",
        "Diego Alfaro - Implementar la función train\n",
        "Mauricio Anguiano - Responsable de implementar GPU para el entrenamiento\n",
        "Jesus Ramirez - Implementar la función sample\n",
        "Luis Adrián - Comentar código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:05.465215Z",
          "iopub.status.busy": "2025-11-28T03:32:05.464298Z",
          "iopub.status.idle": "2025-11-28T03:32:08.911738Z",
          "shell.execute_reply": "2025-11-28T03:32:08.910957Z",
          "shell.execute_reply.started": "2025-11-28T03:32:05.465190Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "m0Bt5-Vw-qp1",
        "outputId": "6a85e74a-f55a-45a8-f78d-93d29bc8a2b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\n",
            "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.913514Z",
          "iopub.status.busy": "2025-11-28T03:32:08.913257Z",
          "iopub.status.idle": "2025-11-28T03:32:08.918335Z",
          "shell.execute_reply": "2025-11-28T03:32:08.917433Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.913491Z"
        },
        "trusted": true,
        "id": "jmL4oZtF-qp3"
      },
      "outputs": [],
      "source": [
        "# Importar las bibliotecas necesarias para construir y entrenar el modelo GPT-2\n",
        "import torch                                    # Biblioteca principal de PyTorch para operaciones con tensores\n",
        "import torch.nn as nn                           # Módulos de redes neuronales (capas, funciones de pérdida, etc.)\n",
        "import torch.nn.functional as F                 # API funcional para funciones de activación, softmax, etc.\n",
        "from torch.utils.data import Dataset, DataLoader  # Utilidades para manejar conjuntos de datos y lotes\n",
        "from datasets import load_dataset              # Biblioteca de Hugging Face para cargar datasets predefinidos\n",
        "import torch.optim as optim                    # Algoritmos de optimización (Adam, SGD, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.919398Z",
          "iopub.status.busy": "2025-11-28T03:32:08.919116Z",
          "iopub.status.idle": "2025-11-28T03:32:08.934717Z",
          "shell.execute_reply": "2025-11-28T03:32:08.933838Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.919362Z"
        },
        "trusted": true,
        "id": "ntqrC9JL-qp4"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    '''\n",
        "    Clase de configuración que contiene todos los hiperparámetros del modelo GPT-2.\n",
        "    Estos parámetros definen la arquitectura y el comportamiento del transformer.\n",
        "\n",
        "    Atributos:\n",
        "        vocab_size: Tamaño del vocabulario (por defecto 50257 para el tokenizador GPT-2)\n",
        "        max_seq_length: Longitud máxima de secuencia que el modelo puede procesar\n",
        "        embed_size: Dimensión de los embeddings de tokens y estados ocultos\n",
        "        num_layers: Número de bloques transformer apilados\n",
        "        num_heads: Número de cabezas de atención en la atención multi-cabeza\n",
        "        dropout: Probabilidad de dropout para regularización\n",
        "    '''\n",
        "    def __init__(self, vocab_size = 50257, max_seq_length = 128, embed_size = 768, num_layers = 12,\n",
        "                 num_heads = 12, dropout = 0.1):\n",
        "        self.vocab_size = vocab_size          # GPT-2 usa tokenización BPE con ~50k tokens\n",
        "        self.max_seq_length = max_seq_length  # Tamaño de la ventana de contexto\n",
        "        self.embed_size = embed_size          # Dimensión oculta (d_model en el paper)\n",
        "        self.num_layers = num_layers          # Profundidad del transformer\n",
        "        self.num_heads = num_heads            # Operaciones de atención en paralelo\n",
        "        self.dropout = dropout                # Regularización para prevenir sobreajuste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.936372Z",
          "iopub.status.busy": "2025-11-28T03:32:08.936116Z",
          "iopub.status.idle": "2025-11-28T03:32:08.953351Z",
          "shell.execute_reply": "2025-11-28T03:32:08.952714Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.936357Z"
        },
        "trusted": true,
        "id": "YVeDTtZz-qp5"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    '''\n",
        "    Mecanismo de Auto-Atención Multi-Cabeza con enmascaramiento causal para generación autoregresiva.\n",
        "\n",
        "    Este es el componente central del transformer que permite a cada token atender\n",
        "    a los tokens anteriores en la secuencia. La máscara causal asegura que las predicciones\n",
        "    para la posición i solo dependan de las posiciones < i (propiedad autoregresiva).\n",
        "\n",
        "    La fórmula de atención es: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
        "    '''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Asegurar que embed_size sea divisible por num_heads para división equitativa\n",
        "        assert config.embed_size % config.num_heads == 0, 'tamaños no compatibles'\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.embed_size // config.num_heads  # Dimensión por cabeza de atención\n",
        "\n",
        "        # Matrices de proyección lineal para Query, Key y Value\n",
        "        # Estas aprenden a proyectar los embeddings de entrada a los espacios Q, K, V\n",
        "        self.W_q = nn.Linear(config.embed_size, config.embed_size)  # Proyección de Query\n",
        "        self.W_k = nn.Linear(config.embed_size, config.embed_size)  # Proyección de Key\n",
        "        self.W_v = nn.Linear(config.embed_size, config.embed_size)  # Proyección de Value\n",
        "        self.output = nn.Linear(config.embed_size, config.embed_size)  # Proyección de salida\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Crear máscara causal (matriz triangular inferior) para prevenir atención a tokens futuros\n",
        "        # Esta máscara se registra como buffer (no como parámetro) para que se guarde con el modelo\n",
        "        # pero no se actualice durante el entrenamiento\n",
        "        self.register_buffer(\n",
        "            'mask',\n",
        "            torch.tril(torch.ones(config.max_seq_length, config.max_seq_length)\n",
        "                      ).view(-1, 1, config.max_seq_length, config.max_seq_length)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_length, embed_dim = x.size()  # B = tamaño de lote, T = longitud de secuencia, D = dimensión de embedding\n",
        "\n",
        "        # Proyectar entrada a Q, K, V y reorganizar para atención multi-cabeza\n",
        "        # Transformación de forma: (B, T, D) -> (B, T, num_heads, head_dim) -> (B, num_heads, T, head_dim)\n",
        "        Q = self.W_q(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcular puntuaciones de atención con producto punto escalado\n",
        "        # QK^T / sqrt(d_k) - el escalado previene que los gradientes se vuelvan muy pequeños\n",
        "        attn = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # Forma: (B, num_heads, T, T)\n",
        "\n",
        "        # Aplicar máscara causal: establecer posiciones futuras a -inf para que softmax les dé probabilidad 0\n",
        "        attn = attn.masked_fill(self.mask[:, :, :seq_length, :seq_length] == 0, float('-inf'))\n",
        "\n",
        "        # Convertir puntuaciones de atención a probabilidades\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Calcular suma ponderada de valores basada en probabilidades de atención\n",
        "        scores = attn @ V  # Forma: (B, num_heads, T, head_dim)\n",
        "\n",
        "        # Concatenar cabezas y proyectar de vuelta a embed_size\n",
        "        # Forma: (B, num_heads, T, head_dim) -> (B, T, num_heads, head_dim) -> (B, T, embed_size)\n",
        "        scores = scores.transpose(1, 2).contiguous().view(batch, seq_length, embed_dim)\n",
        "\n",
        "        return self.dropout(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.954348Z",
          "iopub.status.busy": "2025-11-28T03:32:08.954126Z",
          "iopub.status.idle": "2025-11-28T03:32:08.970170Z",
          "shell.execute_reply": "2025-11-28T03:32:08.969428Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.954333Z"
        },
        "trusted": true,
        "id": "JwhmeWLr-qp6"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    '''\n",
        "    Red Feed-Forward (FFN) - también conocida como bloque MLP en el transformer.\n",
        "\n",
        "    Esta es una red totalmente conectada aplicada posición por posición después de la atención.\n",
        "    Consiste en dos transformaciones lineales con una activación GELU en medio.\n",
        "    La capa oculta expande a 4x el tamaño del embedding, luego proyecta de vuelta.\n",
        "\n",
        "    FFN(x) = GELU(xW1 + b1)W2 + b2\n",
        "\n",
        "    La expansión permite al modelo aprender transformaciones no lineales más complejas.\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Primera capa lineal expande la dimensión por 4x (estándar en GPT-2)\n",
        "        self.fc1 = nn.Linear(config.embed_size, 4 * config.embed_size)\n",
        "        # Activación GELU (Unidad Lineal de Error Gaussiano) - más suave que ReLU\n",
        "        self.gelu = nn.GELU()\n",
        "        # Segunda capa lineal proyecta de vuelta a la dimensión original\n",
        "        self.fc2 = nn.Linear(4 * config.embed_size, config.embed_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Aplicar expansión -> activación -> proyección -> dropout\n",
        "        x = self.fc2(self.gelu(self.fc1(x)))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.971144Z",
          "iopub.status.busy": "2025-11-28T03:32:08.970907Z",
          "iopub.status.idle": "2025-11-28T03:32:08.986639Z",
          "shell.execute_reply": "2025-11-28T03:32:08.985859Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.971122Z"
        },
        "trusted": true,
        "id": "TMq8dmBJ-qp7"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    '''\n",
        "    Bloque Transformer individual que combina Auto-Atención y Red Feed-Forward.\n",
        "\n",
        "    Esto implementa la variante Pre-LayerNorm (usada en GPT-2) donde LayerNorm\n",
        "    se aplica antes de cada sub-capa en lugar de después. Esto proporciona un\n",
        "    entrenamiento más estable comparado con el diseño Post-LayerNorm original.\n",
        "\n",
        "    Estructura:\n",
        "        x -> LayerNorm -> Auto-Atención -> + (residual) -> LayerNorm -> FFN -> + (residual)\n",
        "\n",
        "    Las conexiones residuales ayudan con el flujo de gradientes en redes profundas.\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(config.embed_size)  # Normalizar antes de atención\n",
        "        self.attention = SelfAttention(config)         # Auto-atención multi-cabeza\n",
        "        self.norm2 = nn.LayerNorm(config.embed_size)  # Normalizar antes de FFN\n",
        "        self.mlp = FFN(config)                         # Red feed-forward\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Sub-bloque de atención con conexión residual\n",
        "        # x + Attention(LayerNorm(x))\n",
        "        x = x + self.attention(self.norm1(x))\n",
        "        # Sub-bloque FFN con conexión residual\n",
        "        # x + FFN(LayerNorm(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:08.987659Z",
          "iopub.status.busy": "2025-11-28T03:32:08.987388Z",
          "iopub.status.idle": "2025-11-28T03:32:09.000692Z",
          "shell.execute_reply": "2025-11-28T03:32:08.999933Z",
          "shell.execute_reply.started": "2025-11-28T03:32:08.987636Z"
        },
        "trusted": true,
        "id": "eDYh1n9M-qp8"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    '''\n",
        "    Arquitectura completa del Modelo de Lenguaje GPT-2.\n",
        "\n",
        "    Este modelo combina:\n",
        "    1. Embeddings de tokens - convierte IDs de tokens a vectores densos\n",
        "    2. Embeddings posicionales - codifica información de posición (aprendidos, no sinusoidales)\n",
        "    3. Pila de bloques Transformer - procesa la secuencia\n",
        "    4. Cabeza de modelado de lenguaje - proyecta de vuelta al vocabulario para predicción del siguiente token\n",
        "\n",
        "    Los logits de salida representan la distribución de probabilidad sobre el vocabulario\n",
        "    para el siguiente token en cada posición.\n",
        "    '''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embedding de tokens: mapea índices del vocabulario a vectores densos\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_size)\n",
        "\n",
        "        # Embedding posicional: codificaciones de posición aprendidas (a diferencia del sinusoidal del Transformer original)\n",
        "        self.pos_embed = nn.Embedding(config.max_seq_length, config.embed_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Pila de bloques transformer - aquí es donde ocurre la \"magia\"\n",
        "        self.transformers = nn.Sequential(*[Transformer(config) for _ in range(config.num_layers)])\n",
        "\n",
        "        # Normalización de capa final antes de la proyección de salida\n",
        "        self.norm1 = nn.LayerNorm(config.embed_size)\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        batch, seq_length = input_tokens.size()\n",
        "\n",
        "        # Crear índices de posición [0, 1, 2, ..., seq_length-1]\n",
        "        pos = torch.arange(0, seq_length, dtype=torch.long, device=input_tokens.device).unsqueeze(0)\n",
        "\n",
        "        # Combinar embeddings de tokens y embeddings posicionales\n",
        "        # Esto da a cada token tanto su significado como su información de posición\n",
        "        x = self.token_embed(input_tokens) + self.pos_embed(pos)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pasar a través de todas las capas transformer\n",
        "        x = self.transformers(x)\n",
        "\n",
        "        # Aplicar normalización de capa final\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Proyectar al tamaño del vocabulario usando weight tying (reutilizar pesos del embedding de tokens)\n",
        "        # Esta es una técnica común que reduce parámetros y frecuentemente mejora el rendimiento\n",
        "        # Forma de salida: (batch, seq_length, vocab_size)\n",
        "        return x @ self.token_embed.weight.t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:09.001635Z",
          "iopub.status.busy": "2025-11-28T03:32:09.001434Z",
          "iopub.status.idle": "2025-11-28T03:32:15.951244Z",
          "shell.execute_reply": "2025-11-28T03:32:15.950596Z",
          "shell.execute_reply.started": "2025-11-28T03:32:09.001616Z"
        },
        "trusted": true,
        "id": "N_5tv8PZ-qp8",
        "outputId": "3e806730-0393-4633-bf30-703e07d4234f",
        "colab": {
          "referenced_widgets": [
            "f33056d72cb74340a5e5d42055aec38b",
            "db8cf8653a984f13bbf491711dbd2f16",
            "c6e3f8fc0fac4e71b5c3cb4795dfe196",
            "6c857033cc994ede8ffcd0e1ffd10db6",
            "bd3066bd3f9442a8ae1aba9888ad8a5d"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f33056d72cb74340a5e5d42055aec38b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db8cf8653a984f13bbf491711dbd2f16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6e3f8fc0fac4e71b5c3cb4795dfe196",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c857033cc994ede8ffcd0e1ffd10db6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd3066bd3f9442a8ae1aba9888ad8a5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Importar el tokenizador pre-entrenado de GPT-2 de Hugging Face\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Configurar dispositivo a GPU si está disponible, de lo contrario usar CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Cargar el tokenizador GPT-2 - maneja la conversión de texto a IDs de tokens y viceversa\n",
        "# El tokenizador usa Codificación de Pares de Bytes (BPE) con un vocabulario de ~50,257 tokens\n",
        "tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# Establecer el token de relleno igual al token de fin de secuencia\n",
        "# GPT-2 no tiene un token de relleno dedicado, así que reutilizamos eos_token\n",
        "tokeniser.pad_token = tokeniser.eos_token\n",
        "\n",
        "# Definir longitud de secuencia para entrenamiento (tamaño de ventana de contexto)\n",
        "SEQ_LENGTH = 128\n",
        "\n",
        "# Inicializar la configuración del modelo y crear el modelo GPT-2\n",
        "config = Config(max_seq_length=SEQ_LENGTH)\n",
        "model = GPT2(config).to(device)  # Mover modelo a GPU/CPU\n",
        "\n",
        "# Inicializar optimizador AdamW con tasa de aprendizaje 3e-4\n",
        "# AdamW es Adam con decaimiento de peso apropiado (desacoplado de las actualizaciones de gradiente)\n",
        "optimiser = optim.AdamW(model.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbizAXt-qp9"
      },
      "source": [
        "## Tokenizador GPT-2\n",
        "\n",
        "### ¿Por qué usamos el tokenizador de GPT-2?\n",
        "\n",
        "El **tokenizador GPT-2** utiliza el algoritmo de **Byte-Pair Encoding (BPE)**, que es fundamental para el procesamiento eficiente del lenguaje natural. Las razones principales para usar este tokenizador son:\n",
        "\n",
        "1. **Codificación de Pares de Bytes (BPE)**:\n",
        "   - BPE es un algoritmo de compresión que aprende a dividir palabras en subpalabras frecuentes\n",
        "   - Maneja palabras desconocidas dividiéndolas en subunidades conocidas\n",
        "   - Balance óptimo entre vocabulario de caracteres (muy largo) y vocabulario de palabras (incompleto)\n",
        "\n",
        "2. **Vocabulario de ~50,257 tokens**:\n",
        "   - Tamaño suficiente para capturar la mayoría de patrones lingüísticos\n",
        "   - Incluye tokens para caracteres especiales, puntuación y espacios\n",
        "\n",
        "3. **Compatibilidad**:\n",
        "   - Usar el mismo tokenizador que el modelo original GPT-2 asegura consistencia\n",
        "   - Permite aprovechar el vocabulario pre-entrenado de OpenAI\n",
        "\n",
        "4. **Multilingüe**:\n",
        "   - Aunque fue entrenado principalmente en inglés, BPE puede manejar texto en español\n",
        "   - Las subpalabras permiten representar palabras en otros idiomas\n",
        "\n",
        "### Diagrama del proceso de tokenización:\n",
        "\n",
        "```\n",
        "Texto: \"Hola mundo\"\n",
        "         ↓\n",
        "    [Tokenización BPE]\n",
        "         ↓\n",
        "IDs: [39, 5765, 416, 78, 1597]\n",
        "         ↓\n",
        "    [Embedding]\n",
        "         ↓\n",
        "Vectores densos de dimensión 768\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:32:15.952739Z",
          "iopub.status.busy": "2025-11-28T03:32:15.952133Z",
          "iopub.status.idle": "2025-11-28T03:37:38.040154Z",
          "shell.execute_reply": "2025-11-28T03:37:38.039548Z",
          "shell.execute_reply.started": "2025-11-28T03:32:15.952709Z"
        },
        "trusted": true,
        "id": "wcJzXHhC-qp9",
        "outputId": "57211dd8-a02a-4dc7-99ea-f74dd4d77477",
        "colab": {
          "referenced_widgets": [
            "31ded594141942eb8f6831558347ac65"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31ded594141942eb8f6831558347ac65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/48829 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Cargar y preparar el dataset de texto en español para entrenamiento\n",
        "\n",
        "# URL a un corpus de texto en español del dataset MC4 (C4 multilingüe)\n",
        "# Esta es una versión limpia de datos web de Common Crawl en español\n",
        "data_files = \"https://huggingface.co/datasets/bertin-project/mc4-es-sampled/resolve/main/mc4-es-train-50M-gaussian-shard-0001-of-1024.json.gz\"\n",
        "\n",
        "# Cargar el dataset usando la biblioteca datasets de Hugging Face\n",
        "# El constructor 'json' parsea el archivo JSON comprimido\n",
        "# split=\"train[:100%]\" carga todo el split de entrenamiento\n",
        "dataset = load_dataset(\"json\", data_files=data_files, split=\"train[:100%]\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    '''\n",
        "    Tokeniza ejemplos de texto en IDs de tokens.\n",
        "\n",
        "    Args:\n",
        "        examples: Diccionario conteniendo el campo 'text' con cadenas de texto crudo\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con 'input_ids' conteniendo secuencias tokenizadas\n",
        "    '''\n",
        "    # Tokenizar, truncar a longitud máxima y rellenar secuencias más cortas\n",
        "    return tokeniser(examples[\"text\"], truncation=True, max_length=SEQ_LENGTH, padding=\"max_length\")\n",
        "\n",
        "# Aplicar tokenización a todo el dataset (en lotes para eficiencia)\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Establecer el formato del dataset a tensores de PyTorch y mantener solo input_ids\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "# Crear un DataLoader para dividir en lotes y mezclar durante el entrenamiento\n",
        "# batch_size=8: Procesar 8 secuencias a la vez\n",
        "# shuffle=True: Aleatorizar el orden en cada época para mejor generalización\n",
        "loader = DataLoader(tokenized_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XrxDDWb-qp-"
      },
      "source": [
        "## Dataset MC4 en Español\n",
        "\n",
        "### ¿Por qué usamos el dataset MC4-ES?\n",
        "\n",
        "El **MC4 (Multilingual Colossal Clean Crawled Corpus)** es una versión multilingüe del dataset C4, específicamente diseñado para entrenar modelos de lenguaje. Elegimos la versión en español por las siguientes razones:\n",
        "\n",
        "1. **Datos de alta calidad**:\n",
        "   - Proviene de Common Crawl, pero con limpieza extensiva\n",
        "   - Filtrado para remover contenido duplicado, texto de baja calidad y spam\n",
        "   - Mantenido por el proyecto BERTIN, especializado en NLP en español\n",
        "\n",
        "2. **Volumen de datos**:\n",
        "   - Contiene millones de textos en español\n",
        "   - Suficiente para entrenar un modelo de lenguaje desde cero\n",
        "   - Variedad de dominios: noticias, blogs, wikipedia, etc.\n",
        "\n",
        "3. **Formato accesible**:\n",
        "   - Disponible en Hugging Face Datasets\n",
        "   - Formato JSON comprimido para descarga eficiente\n",
        "   - Fácil integración con el ecosistema de PyTorch\n",
        "\n",
        "4. **Relevancia para el proyecto**:\n",
        "   - Entrenar con texto en español demuestra la capacidad del modelo\n",
        "   - Permite generar texto coherente en nuestro idioma\n",
        "   - Útil para aplicaciones de NLP en español\n",
        "\n",
        "### Estructura del dataset:\n",
        "\n",
        "| Campo | Descripción |\n",
        "|-------|-------------|\n",
        "| `text` | Contenido textual del documento |\n",
        "| `url` | URL de origen del texto |\n",
        "| `timestamp` | Fecha de extracción |\n",
        "\n",
        "### Pipeline de procesamiento:\n",
        "\n",
        "```\n",
        "URL del dataset (JSON.gz)\n",
        "         ↓\n",
        "    [load_dataset]\n",
        "         ↓\n",
        "    Dataset crudo\n",
        "         ↓\n",
        "    [tokenize_function]\n",
        "         ↓\n",
        "    Secuencias de IDs\n",
        "         ↓\n",
        "    [DataLoader]\n",
        "         ↓\n",
        "    Lotes para entrenamiento\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:37:38.042449Z",
          "iopub.status.busy": "2025-11-28T03:37:38.042227Z",
          "iopub.status.idle": "2025-11-28T03:37:38.047851Z",
          "shell.execute_reply": "2025-11-28T03:37:38.047300Z",
          "shell.execute_reply.started": "2025-11-28T03:37:38.042431Z"
        },
        "trusted": true,
        "id": "zkXfljPg-qp-"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimiser, epochs=10):\n",
        "    '''\n",
        "    Bucle de entrenamiento para el modelo de lenguaje GPT-2.\n",
        "\n",
        "    Usa el objetivo estándar de modelado de lenguaje: predecir el siguiente token\n",
        "    dados todos los tokens anteriores. Esto es entrenamiento autoregresivo.\n",
        "\n",
        "    Args:\n",
        "        model: El modelo GPT-2 a entrenar\n",
        "        loader: DataLoader que proporciona lotes de texto tokenizado\n",
        "        optimiser: Optimizador para actualizar los pesos del modelo\n",
        "        epochs: Número de pasadas completas a través del dataset\n",
        "    '''\n",
        "    model.train()  # Establecer modelo en modo entrenamiento (habilita dropout, etc.)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch in loader:\n",
        "            # Mover tokens de entrada al dispositivo apropiado (GPU/CPU)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "\n",
        "            # Desplazamiento para entrenamiento autoregresivo:\n",
        "            # Entrada: tokens [0, 1, 2, ..., n-1] (todos menos el último)\n",
        "            # Objetivo: tokens [1, 2, 3, ..., n] (todos menos el primero)\n",
        "            # Esto enseña al modelo a predecir cada siguiente token\n",
        "            inputs = input_ids[:, :-1]   # Todo excepto el último token\n",
        "            targets = input_ids[:, 1:]   # Todo excepto el primer token\n",
        "\n",
        "            optimiser.zero_grad()  # Limpiar gradientes del paso anterior\n",
        "\n",
        "            # Paso forward: obtener logits para cada posición\n",
        "            logits = model(inputs)  # Forma: (batch, seq_length-1, vocab_size)\n",
        "\n",
        "            # Calcular pérdida de entropía cruzada\n",
        "            # Aplanar logits y objetivos para la función de pérdida\n",
        "            # logits: (batch * seq_length, vocab_size)\n",
        "            # targets: (batch * seq_length,)\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "\n",
        "            # Paso backward: calcular gradientes\n",
        "            loss.backward()\n",
        "\n",
        "            # Actualizar pesos usando los gradientes calculados\n",
        "            optimiser.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Imprimir pérdida promedio para esta época\n",
        "        print(f\"Época {epoch+1}/{epochs}, Pérdida: {total_loss/len(loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T03:37:38.048683Z",
          "iopub.status.busy": "2025-11-28T03:37:38.048448Z",
          "iopub.status.idle": "2025-11-28T06:00:17.365939Z",
          "shell.execute_reply": "2025-11-28T06:00:17.365134Z",
          "shell.execute_reply.started": "2025-11-28T03:37:38.048660Z"
        },
        "trusted": true,
        "id": "eizqUvBV-qp_",
        "outputId": "023f48c3-d113-4d13-cb1a-10fc1ecc7dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 8.1781\n",
            "Epoch 2/10, Loss: 4.6126\n",
            "Epoch 3/10, Loss: 4.0241\n",
            "Epoch 4/10, Loss: 3.6984\n",
            "Epoch 5/10, Loss: 3.5213\n",
            "Epoch 6/10, Loss: 3.3912\n",
            "Epoch 7/10, Loss: 3.2870\n",
            "Epoch 8/10, Loss: 3.1976\n",
            "Epoch 9/10, Loss: 3.1184\n",
            "Epoch 10/10, Loss: 3.0457\n"
          ]
        }
      ],
      "source": [
        "# Iniciar entrenamiento del modelo por 10 épocas\n",
        "# Esto tomará algo de tiempo dependiendo del tamaño del dataset y el hardware\n",
        "train(model, loader, optimiser, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T06:00:58.964066Z",
          "iopub.status.busy": "2025-11-28T06:00:58.963431Z",
          "iopub.status.idle": "2025-11-28T06:00:59.395039Z",
          "shell.execute_reply": "2025-11-28T06:00:59.394305Z",
          "shell.execute_reply.started": "2025-11-28T06:00:58.964035Z"
        },
        "trusted": true,
        "id": "QF2oj6q7-qp_",
        "outputId": "86ebe0b1-1ef0-4a5e-972f-a93890e89401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 3118,  1556,   463,  3014,    68,   390,  6253,  4533, 17605,    68,\n",
            "          2382,   291,  5733,  8591,  4902,   304,    73,   721,   315, 12151,\n",
            "           390,  8591, 26986, 32482,  1619, 26482,   930,  2295,   349,    13,\n",
            "           785,   198,  3118,  1556,   463,  3014,    68,   390,  6253,  4533,\n",
            "         17605,    68,  2382,   291,  5733,  8591,  4902,   304,    73,   721,\n",
            "           315, 12151,   390,  8591, 26986, 32482,  1619, 26482]],\n",
            "       device='cuda:0')\n",
            "Un estudiante de doctorado norteamericano la crisis ejecutiva de la Universidad del Salvador | Emol.com\n",
            "Un estudiante de doctorado norteamericano la crisis ejecutiva de la Universidad del Salvador\n"
          ]
        }
      ],
      "source": [
        "def sample(model, device, tokenizer, prompt, length=50, temperature=1.0):\n",
        "    '''\n",
        "    Genera texto de forma autoregresiva desde el modelo entrenado.\n",
        "\n",
        "    Comenzando desde un prompt, el modelo predice un token a la vez,\n",
        "    agregando cada token predicho a la secuencia y repitiendo.\n",
        "\n",
        "    Args:\n",
        "        model: Modelo GPT-2 entrenado\n",
        "        device: Dispositivo para ejecutar inferencia (cuda/cpu)\n",
        "        tokenizer: Tokenizador para codificar/decodificar texto\n",
        "        prompt: Texto inicial desde donde comenzar la generación\n",
        "        length: Número de nuevos tokens a generar\n",
        "        temperature: Controla la aleatoriedad (1.0 = normal, <1 = más enfocado, >1 = más aleatorio)\n",
        "\n",
        "    Returns:\n",
        "        Texto generado como cadena\n",
        "    '''\n",
        "    model.eval()  # Establecer modelo en modo evaluación (deshabilita dropout)\n",
        "\n",
        "    # Codificar el prompt en IDs de tokens\n",
        "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Generar tokens uno a la vez (generación autoregresiva)\n",
        "    for _ in range(length):\n",
        "        # Solo usar los últimos SEQ_LENGTH tokens si la secuencia se vuelve muy larga\n",
        "        tokens_cond = tokens[:, -SEQ_LENGTH:]\n",
        "\n",
        "        # Obtener predicciones del modelo sin calcular gradientes (más rápido)\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens_cond)\n",
        "\n",
        "        # Obtener logits para la última posición y aplicar escalado de temperatura\n",
        "        # Mayor temperatura = más aleatorio, menor = más determinístico\n",
        "        next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Muestrear de la distribución de probabilidad (no argmax para diversidad)\n",
        "        # Esto da variedad en el texto generado en lugar de siempre elegir el token más probable\n",
        "        next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "\n",
        "        # Agregar el token predicho a la secuencia\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    print(tokens)  # Imprimir IDs de tokens crudos para depuración\n",
        "\n",
        "    # Decodificar todos los tokens de vuelta a texto\n",
        "    return tokenizer.decode(tokens[0])\n",
        "\n",
        "# Ejemplo de uso: Generar texto comenzando con un prompt en español\n",
        "print(sample(model, device, tokeniser, prompt=\"Un estudiante de doctorado\", length=50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-28T06:01:04.068103Z",
          "iopub.status.busy": "2025-11-28T06:01:04.067843Z",
          "iopub.status.idle": "2025-11-28T06:01:08.084912Z",
          "shell.execute_reply": "2025-11-28T06:01:08.084157Z",
          "shell.execute_reply.started": "2025-11-28T06:01:04.068084Z"
        },
        "trusted": true,
        "id": "yPGUseFj-qp_",
        "outputId": "8b675b14-d5d2-42e4-eec7-1f6c67c51a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sentence 1 ---\n",
            "Prompt: Un ingeniero\n",
            "tensor([[ 3118, 27016,   959,    78,   285, 40138,   551,   325, 12654,   283,\n",
            "           257,   424,  2614, 32482,  2968,   257,  9223,   544,   532, 20442,\n",
            "            78,  4496,  2947,   198,  1415,  2362, 26597,  4679,  1853, 16964,\n",
            "          2297, 44456, 18840,   978, 40407,    78,    13,  2398,   198,  9527,\n",
            "         27016,   959,    78,  2604,    81, 10205,   435, 46436, 23593,   257,\n",
            "         21733,   395,  8847,  1145]], device='cuda:0')\n",
            "Generated: Un ingeniero más enseñar a su personalidad popular a Rusia - Mendoza Post\n",
            "14 Septiembre 2015 por Redacción Al Socialismo.org\n",
            "El ingeniero logró al positivo a Nuestras fam\n",
            "\n",
            "--- Sentence 2 ---\n",
            "Prompt: La inteligencia artificial\n",
            "tensor([[14772, 33649,  9324, 33743, 11666,  1619,  6599, 27799, 31215,  8591,\n",
            "         24676, 28778,  6599, 27799,   355,   396, 29634,  1619,  6599, 27799,\n",
            "          8358,   277,  6765,   979, 10205,   401,    78,  5259,   934, 15818,\n",
            "           551,  1288,  3175, 48711,   575,   471, 19555,    12, 11280,    13,\n",
            "           274,   532,  1892,   291,  4448,   198, 14772, 33649,  9324, 33743,\n",
            "         11666,  1619,  6599, 27799, 31215]], device='cuda:0')\n",
            "Generated: La inteligencia artificial del PSOE para la PUERN PSOE asistencia del PSOE que falleció como titular provincial en el Palacio Y UGT-Link.es - Noticias\n",
            "La inteligencia artificial del PSOE para\n",
            "\n",
            "--- Sentence 3 ---\n",
            "Prompt: El futuro de la tecnología\n",
            "tensor([[ 9527, 13294,  1434,   390,  8591,   573, 31522,   928, 29690,   390,\n",
            "          8854,   198,    36,  4169,  2401, 32735, 38836,  4533,    11,  2457,\n",
            "           434,    68,   384,  4681, 10205,  8591,  2336,  5330,   390,  1556,\n",
            "            64,  1779, 44070, 18840,   390,  1814,  7079,  2256, 38982,  6557,\n",
            "           358,  8471,   300,   518,  2188,   390,   555,    64,  2984,    64,\n",
            "         11129, 35764, 18840,   556,    81,  4763,    13, 10884, 38286]],\n",
            "       device='cuda:0')\n",
            "Generated: El futuro de la tecnología de 73\n",
            "Este domingo pasado, finalmente se celebró la figura de esta condición de fundadorhead Fernández luego de una misaelección agrada. Sin embargo\n",
            "\n",
            "--- Sentence 4 ---\n",
            "Prompt: En el año 2050\n",
            "tensor([[ 4834,  1288,   257, 31329, 32215,    11,  1288, 38836,  4533, 18842,\n",
            "           390,   730,  4679,   305,   532,  6599, 31667,   198,  4834,  1288,\n",
            "           257, 31329, 32215,    11,  1288,   288, 29690,  1542,   390,   730,\n",
            "          4679,   305,    11,   360, 29690,  1542,   390,   730,  4679,   305,\n",
            "            11,  1288,   449, 22521,  1619,  1224,  7639,   748,   433,   291,\n",
            "         43348,  5347,   829,   331,   379]], device='cuda:0')\n",
            "Generated: En el año 2050, el pasado mes de febrero - PSUV\n",
            "En el año 2050, el día 30 de febrero, Día 30 de febrero, el Jefe del complejo desarticulo ingles y at\n",
            "\n",
            "--- Sentence 5 ---\n",
            "Prompt: Los científicos descubrieron\n",
            "tensor([[28903,   269,  1153,  8836,    69,   291,   418,  1715,   549,  5277,\n",
            "           261,  1288,   264,   396, 19687,  6863,   265, 23593,   390, 22346,\n",
            "          1556,   463,  3014,   274,   289, 27112,   418, 32317,   320, 37503,\n",
            "           532,  2294,  6557,   198, 28489, 29690,  8570,   330,  1538,   930,\n",
            "          2310,    14,  2713,    14,  5304,   198, 28903,   269,  1153,  8836,\n",
            "            69,   291,   418,  1715,   549,  5277,   261,  1288,   264,   396,\n",
            "         19687]], device='cuda:0')\n",
            "Generated: Los científicos descubrieron el sistema administrativo de los estudiantes huidos pesimistas - Canadá\n",
            "Economía delegacional | 21/05/2016\n",
            "Los científicos descubrieron el sistema\n",
            "\n",
            "--- Sentence 6 ---\n",
            "Prompt: La universidad ofrece\n",
            "tensor([[14772,  5820, 32482,   286,   260,   344,   555,    64,   581,   349,\n",
            "         42008, 18840,   390,   379,   298, 22484,   930, 10123, 18893,   274,\n",
            "         12381,    79,  6557, 13370,   292,   390,  8591,  6924,  1437, 15531,\n",
            "         12523,   390, 17456,   198,    51,   411, 29489,   452,   418,  1618,\n",
            "           528,   272,   257,   355,   396, 29634,   551,  1288, 13588,    78,\n",
            "          5616,  1192, 15666,   331,   360, 13137]], device='cuda:0')\n",
            "Generated: La universidad ofrece una resolución de atentados | Lasvocesdelpáticas de la medicina Criminalista de Chile\n",
            "Tres deportivos organizan a asistencia en el Circuito Melancourt y DPN\n",
            "\n",
            "--- Sentence 7 ---\n",
            "Prompt: El presidente anunció\n",
            "tensor([[ 9527,  1893,    68,   281, 49652, 10205,  8358,   555,   795,  3866,\n",
            "           358,   320,  1153,    78,  1430,    64,  1619,   269,  7056,   311,\n",
            "            13,    43,  1539,  4643, 18840,  3970,  1619,   371,  8836,    78,\n",
            "            13, 15831,    78,   406, 22436,   930,  2547,    64,   327,  6557,\n",
            "            67,   528,   384,   401, 29188,   390,   331,   418,   299,   518,\n",
            "            85,   418,   369,   267,  2213,   292]], device='cuda:0')\n",
            "Generated: El presidente anunció que un emprendimiento programa del caza S.L., Verónica del Río. Solido Lohan | Para Cádiz se comenta de yos nuevos con otras\n",
            "\n",
            "--- Sentence 8 ---\n",
            "Prompt: Durante la conferencia\n",
            "tensor([[36927, 12427,  8591,  1013,   567, 10782,   544,   390,   662,  5907,\n",
            "            64,    11,  1288,  6924,  3263,    78,  1305, 36274,  8836,    69,\n",
            "          3713,   331,  1288,  7401,    78,   951,  2381, 10115,    11, 25021,\n",
            "          4267,   269,   961,  2850,   331,  9591,  1013,  9100,   274,  5391,\n",
            "           418,   662,  5907,    64,    13,   198,  5005,  1556,    78,   551,\n",
            "         22346,  6184,   223,   782,   417,   274,    11]], device='cuda:0')\n",
            "Generated: Durante la conferencia de prensa, el medicamento Frigorífico y el talento colombiano, beneficios creadores y mayor conferentes dimos prensa.\n",
            "De esto en los Ángeles,\n",
            "\n",
            "--- Sentence 9 ---\n",
            "Prompt: Los investigadores encontraron\n",
            "tensor([[28903,  2009,   324,  2850,  2207,   756,    81,  8045, 22346,  2216,\n",
            "         22484,  1876,  1229,  2596,   257,  8591,  1033,  8154,    72, 18840,\n",
            "          8358, 25568, 29634, 31215,   662,   574,   343,  4818,   418, 17730,\n",
            "           784,   412,  4443,   330,   377,  3565,   331,  1288, 33324,    78,\n",
            "           198, 16060, 13355,  2580, 35657, 13355, 10123,   719,  1699,  2367,\n",
            "         13355, 10123,   719,  1699,  2367, 13355, 10123,   719]],\n",
            "       device='cuda:0')\n",
            "Generated: Los investigadores encontraron los significados involucran a la expresidentión que evidencia para prevenir datos» – Espectaculares y el Mundo\n",
            "Home » Cheuni » Las actividades » Las actividades » Las act\n",
            "\n",
            "--- Sentence 10 ---\n",
            "Prompt: El nuevo sistema\n",
            "tensor([[ 9527,   299,   518, 13038,   264,   396, 19687,   390,   491,   397,\n",
            "         34944,  1188,  5799,   390,  8591,  1145, 17517,  7904,  5401, 34814,\n",
            "           418,   357,    17,     8,   198, 25877, 11649, 36088,   198, 22844,\n",
            "            42,  1847,  8322,  1921, 26720,  5673,    13,   198, 18449,   324,\n",
            "            28,  2718,    13,  9479,    14, 12480,   305,    14, 12832,  6557,\n",
            "         33563,   292,  4533,   551,  8591,  1145, 17517]], device='cuda:0')\n",
            "Generated: El nuevo sistema de trabajo valora de la familia :: Los Grillos (2)\n",
            "CourShare804\n",
            " TrinityKALUDAS MAYMA.\n",
            "eledad=37.jpg/negro/Acágrimasado en la familia\n"
          ]
        }
      ],
      "source": [
        "# Generar 10 oraciones usando diferentes prompts en español\n",
        "# Esto demuestra la capacidad del modelo para continuar texto desde varios puntos de partida\n",
        "\n",
        "prompts = [\n",
        "    \"Un ingeniero\",\n",
        "    \"La inteligencia artificial\",\n",
        "    \"El futuro de la tecnología\",\n",
        "    \"En el año 2050\",\n",
        "    \"Los científicos descubrieron\",\n",
        "    \"La universidad ofrece\",\n",
        "    \"El presidente anunció\",\n",
        "    \"Durante la conferencia\",\n",
        "    \"Los investigadores encontraron\",\n",
        "    \"El nuevo sistema\"\n",
        "]\n",
        "\n",
        "# Generar y mostrar texto para cada prompt\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    print(f\"\\n--- Oración {i} ---\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    # Generar 50 tokens de continuación para cada prompt\n",
        "    print(f\"Generado: {sample(model, device, tokeniser, prompt=prompt, length=50)}\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}